# GovDirections Scraper Fix - Complete Implementation

## Overview

The govdirections scraper has been completely rewritten to address the "inactive source" issue. It now includes login functionality, IT category filtering, and comprehensive data extraction from detail pages.

---

## What Changed

### ‚ùå OLD BEHAVIOR (Before Fix)

**The scraper was:**
1. Going to homepage without login (public access)
2. Scraping whatever was on the first page
3. Only extracting: title, location, closing date from list view
4. Not filtering by category
5. Limited data fields

**Why it failed:**
- No authentication ‚Üí limited access to opportunities
- No category filtering ‚Üí irrelevant results
- Shallow data extraction ‚Üí poor quality
- Missing key fields like description, external links, issuer

---

## ‚úÖ NEW BEHAVIOR (After Fix)

### **1. LOGIN FUNCTIONALITY** (Lines 13-31)

```typescript
async function login(page: Page, user: string, pass: string) {
  // Navigate to homepage
  await page.goto("https://govdirections.com/");

  // Click login button
  await page.click(".btn.btn-default a[href='/users/login']");
  await page.waitForSelector("input[name='username']");

  // Fill credentials
  await page.fill("input[name='username']", user);
  await page.fill("input[name='password']", pass);
  await page.click("button[type='submit'], input[type='submit']");

  // Wait for login to complete
  await page.waitForTimeout(3000);
}
```

**What it does:**
- ‚úÖ Goes to homepage
- ‚úÖ Clicks "Click to LOG IN Below" button
- ‚úÖ Fills username from `GOVEDIRECTIONS_USER` env variable
- ‚úÖ Fills password from `GOVEDIRECTIONS_PASS` env variable
- ‚úÖ Submits form and waits for authentication

---

### **2. IT CATEGORY FILTERING** (Lines 33-44)

```typescript
async function searchITOpportunities(page: Page) {
  // Wait for industries dropdown
  await page.waitForSelector("select[name='industries[]']");

  // Select "IT: Support Services, Help Desk" option (value="940")
  await page.selectOption("select[name='industries[]']", "940");

  // Click search button
  await page.click("input.btn.btn-primary[type='submit'][value='Search']");

  await page.waitForTimeout(2000);
}
```

**What it does:**
- ‚úÖ Finds industries dropdown: `select[name='industries[]']`
- ‚úÖ Selects option with value `"940"` (IT: Support Services, Help Desk)
- ‚úÖ Clicks search button to filter results
- ‚úÖ Only shows IT-related opportunities

---

### **3. DETAIL PAGE EXTRACTION** (Lines 46-137)

```typescript
async function processDetailPage(detailPage: Page, env) {
  // Extract comprehensive data from detail page

  // Title (from h2)
  const title = titleText.replace(/Save this Bid/g, "").trim();

  // Event Date (closing date)
  const eventDateDd = detailPage.locator('dt:has-text("Event Date:") + dd');
  closingDate = await eventDateDd.innerText();

  // External Links (SAM.gov)
  const linkEl = detailPage.locator(
    'dt:has-text("If online, then documents are here:") + dd a'
  );
  externalLink = await linkEl.getAttribute("href");

  // Description
  const descSection = detailPage.locator(
    'h3:has-text("Summary Information") ~ p'
  );
  description = await descSection.first().innerText();

  // Reference Number
  const refEl = detailPage.locator(
    'dt:has-text("reference for this notice") + dd'
  );
  referenceNum = await refEl.innerText();

  // Agency/Sponsor (Issuer)
  const agencyEl = detailPage.locator(
    'dt:has-text("agency sponsor") + dd a'
  );
  issuer = await agencyEl.innerText();

  // Contact Information
  const contactPhoneEl = detailPage.locator(
    'dt:has-text("Agency Contact Information") + dd'
  );
  contactInfo = await contactPhoneEl.innerText();

  return sol;
}
```

**What it extracts:**
- ‚úÖ **Title** - From `<h2>` tag (cleaned of button text)
- ‚úÖ **Closing Date** - From "Event Date" field
- ‚úÖ **External Links** - SAM.gov URLs for documents
- ‚úÖ **Description** - Summary information section
- ‚úÖ **Reference Number** - Opportunity reference/ID
- ‚úÖ **Issuer** - Agency/sponsor name
- ‚úÖ **Contact Info** - Phone, email, contact person
- ‚úÖ **Site URL** - Detail page URL
- ‚úÖ **Site ID** - Extracted from URL

---

### **4. ENHANCED processRow()** (Lines 139-202)

```typescript
async function processRow(row: Locator, env, context: BrowserContext) {
  // Find link in row
  const siteLink = await row.locator("td:nth-child(1) a[href]").first();

  // Open detail page in new tab (Ctrl+Click)
  const newPagePromise = context.waitForEvent("page");
  await siteLink.click({ modifiers: ["Control"] });
  const detailPage = await newPagePromise;
  await detailPage.waitForLoadState();

  // Extract data from detail page
  const sol = await processDetailPage(detailPage, env);
  await detailPage.close();

  // Check expiration, duplicates, and save
  // ... existing validation logic ...
}
```

**What it does:**
- ‚úÖ Opens each opportunity in a new tab
- ‚úÖ Extracts comprehensive data
- ‚úÖ Closes tab after extraction
- ‚úÖ Continues with validation and saving

---

### **5. UPDATED run() FUNCTION** (Lines 255-307)

```typescript
export async function run(page, env, context) {
  const USER = env.DEV_GOVEDIRECTIONS_USER!;
  const PASS = env.DEV_GOVEDIRECTIONS_PASS!;

  // Validate credentials
  if (!USER) throw new Error("Missing USER environment variable");
  if (!PASS) throw new Error("Missing PASS environment variable");

  // Login first
  await login(page, USER, PASS);

  // Search for IT opportunities
  await searchITOpportunities(page);

  // Scrape all results
  const currSols = await scrapeAllSols(page, {...}, context);

  return results;
}
```

**Execution order:**
1. ‚úÖ Validate credentials exist
2. ‚úÖ Login to govdirections
3. ‚úÖ Filter to IT category (value=940)
4. ‚úÖ Scrape all pages
5. ‚úÖ For each row: open detail page, extract data, close tab
6. ‚úÖ Validate and save to database

---

## Environment Variables Required

Add these to your `.env` file:

```bash
# GovDirections Credentials
GOVEDIRECTIONS_USER=your_username
GOVEDIRECTIONS_PASS=your_password

# Or use DEV_ prefix
DEV_GOVEDIRECTIONS_USER=your_username
DEV_GOVEDIRECTIONS_PASS=your_password
```

**Note:** Variable name is `GOVEDIRECTIONS` (no "N" in middle), not `GOVDIRECTIONS`

---

## Data Fields Extracted

### Before Fix (3 fields):
- title
- location
- closingDate

### After Fix (9 fields):
- ‚úÖ **title** - Opportunity title
- ‚úÖ **description** - Full description from summary
- ‚úÖ **issuer** - Agency/sponsor name
- ‚úÖ **closingDate** - Event/due date
- ‚úÖ **contactInfo** - Phone, email, contact person
- ‚úÖ **externalLinks[]** - Array of SAM.gov URLs
- ‚úÖ **site** - "govdirections"
- ‚úÖ **siteUrl** - Detail page URL
- ‚úÖ **siteId** - "govdirections-{id}"
- ‚úÖ **siteData.referenceNum** - Reference number

---

## Testing the Fixed Scraper

### Run Test File:
```bash
cd functions
npx tsx src/test-govdirections.ts
```

### What You'll See:
```
üöÄ Starting govdirections test with Browserbase...
üìù Environment check:
  BASE_URL: https://reconrfp.cendien.com
  SERVICE_KEY: ‚úì Set
  GOVEDIRECTIONS_USER: ‚úì Set
  GOVEDIRECTIONS_PASS: ‚úì Set
  BROWSERBASE_KEY: ‚úì Set

üåê Creating Browserbase session...
‚úì Session created: 12345...
üìπ Watch live: https://www.browserbase.com/sessions/12345...

[Scraper runs...]

‚úÖ Test completed successfully!
Results: {
  "sols": ["sol_abc123", "sol_def456", ...],
  "counts": {
    "success": 15,
    "fail": 0,
    "dup": 3,
    "junk": 2
  }
}

üìπ Review session: https://www.browserbase.com/sessions/12345...
```

### Watch Live:
Click the Browserbase session URL to watch the scraper in real-time:
1. See it login
2. See it select IT category
3. See it open detail pages
4. See data being extracted

---

## Expected Behavior

### Step-by-Step Execution:

**1. Login Phase:**
```
‚Üí Navigate to https://govdirections.com/
‚Üí Click "Click to LOG IN Below" button
‚Üí Fill username field
‚Üí Fill password field
‚Üí Submit form
‚Üí Wait 3 seconds for authentication
```

**2. Search Phase:**
```
‚Üí Wait for industries dropdown
‚Üí Select option value="940" (IT: Support Services, Help Desk)
‚Üí Click "Search" button
‚Üí Wait 2 seconds for results to load
```

**3. Scraping Phase:**
```
For each page:
  ‚Üí Wait for table#bidTable
  ‚Üí Find all rows with class containing "Row"
  ‚Üí For each row:
    ‚Üí Ctrl+Click to open in new tab
    ‚Üí Extract: title, date, description, issuer, contact, links
    ‚Üí Close tab
    ‚Üí Check expiration
    ‚Üí Check duplicate in Firestore
    ‚Üí Save to database if valid
  ‚Üí Check if 30+ expired (early exit)
  ‚Üí Click next page button
  ‚Üí Repeat until no more pages
```

**4. Completion:**
```
‚Üí Log summary: success, fail, duplicates, junk counts
‚Üí Return array of saved solicitation IDs
```

---

## Why This Fixes the "Inactive Source" Issue

### Problem:
- Govdirections was listed as **"Inactive Source"**
- Not being scraped in last 7 days
- Likely returning zero results

### Root Causes Fixed:
1. ‚úÖ **No login** ‚Üí Now logs in with credentials
2. ‚úÖ **No filtering** ‚Üí Now filters to IT category only
3. ‚úÖ **Shallow data** ‚Üí Now extracts comprehensive detail page data
4. ‚úÖ **Wrong selectors** ‚Üí Updated to match current website structure
5. ‚úÖ **Missing credentials** ‚Üí Added credential validation

### Expected Improvements:
- üéØ **Access to logged-in content** ‚Üí More opportunities visible
- üéØ **Relevant results** ‚Üí Only IT opportunities (category 940)
- üéØ **Better data quality** ‚Üí Descriptions, links, contact info
- üéØ **Higher match rate** ‚Üí More fields for duplicate detection
- üéØ **Consistent scraping** ‚Üí Proper authentication prevents blocks

---

## Comparison: Before vs After

| Aspect | Before | After |
|--------|--------|-------|
| **Login** | ‚ùå No | ‚úÖ Yes |
| **Category Filter** | ‚ùå No | ‚úÖ IT: Support Services (940) |
| **Detail Pages** | ‚ùå No | ‚úÖ Opens each in new tab |
| **Data Fields** | 3 fields | 9 fields |
| **External Links** | ‚ùå No | ‚úÖ SAM.gov URLs |
| **Description** | ‚ùå No | ‚úÖ Full summary |
| **Issuer** | ‚ùå No | ‚úÖ Agency name |
| **Contact Info** | ‚ùå No | ‚úÖ Phone, email |
| **Reference Number** | ‚ùå No | ‚úÖ Yes |
| **Data Quality** | üî¥ Low | üü¢ High |
| **Match Rate** | üî¥ Low | üü¢ High |

---

## HTML Selectors Used

### Login Page:
```css
.btn.btn-default a[href='/users/login']  /* Login button */
input[name='username']                    /* Username field */
input[name='password']                    /* Password field */
button[type='submit']                     /* Submit button */
```

### Search Page:
```css
select[name='industries[]']                              /* Industries dropdown */
input.btn.btn-primary[type='submit'][value='Search']    /* Search button */
```

### List Page:
```css
table#bidTable                          /* Results table */
tbody tr[class*='Row']                  /* Table rows */
td:nth-child(1) a[href]                /* Opportunity link */
```

### Detail Page:
```css
h2                                                              /* Title */
dt:has-text("Event Date:") + dd                                /* Closing date */
dt:has-text("If online, then documents are here:") + dd a     /* External link */
h3:has-text("Summary Information") ~ p                        /* Description */
dt:has-text("reference for this notice") + dd                 /* Reference # */
dt:has-text("agency sponsor") + dd a                          /* Issuer */
dt:has-text("Agency Contact Information") + dd                /* Contact info */
```

---

## Troubleshooting

### Issue: Login fails
**Check:**
- Credentials are correct in `.env` file
- Variable name is `GOVEDIRECTIONS_USER` (no "N")
- Login button selector still matches: `.btn.btn-default a[href='/users/login']`

### Issue: No results after search
**Check:**
- IT category option value is still "940"
- Search button selector matches: `input[type='submit'][value='Search']`
- Website didn't change category values

### Issue: Detail pages not opening
**Check:**
- Link selector matches: `td:nth-child(1) a[href]`
- Ctrl+Click is working in Browserbase
- Detail pages load successfully

### Issue: Missing data fields
**Check:**
- Detail page HTML structure hasn't changed
- Selectors for `dt`/`dd` pairs still match
- `has-text()` locators match current text

---

## Next Steps

1. ‚úÖ **Test the scraper** with `npx tsx src/test-govdirections.ts`
2. ‚úÖ **Watch Browserbase session** to verify each step
3. ‚úÖ **Check database** for new govdirections solicitations
4. ‚úÖ **Verify data quality** - descriptions, links, contact info present
5. ‚úÖ **Monitor in production** - should no longer be "inactive"

---

## Success Metrics

After deployment, you should see:

- üéØ **Govdirections appears in "active sources"**
- üéØ **Solicitations scraped in last 7 days > 0**
- üéØ **Higher success count** (more valid opportunities)
- üéØ **Lower junk count** (better filtering to IT category)
- üéØ **Richer data** (descriptions, external links, contacts filled in)
- üéØ **Better duplicate detection** (more fields to match on)

---

## Files Modified

1. ‚úÖ `functions/src/playwright/rfpSearch/govdirections/sols.ts` - Main scraper
2. ‚úÖ `functions/src/test-govdirections.ts` - Test file (updated env vars)

---

## Summary

The govdirections scraper is now a **comprehensive, authenticated scraper** that:
- ‚úÖ Logs in with credentials
- ‚úÖ Filters to IT category only
- ‚úÖ Extracts detailed information from each opportunity
- ‚úÖ Saves rich data for better matching and user experience

This should resolve the "inactive source" issue and provide much better data quality.
